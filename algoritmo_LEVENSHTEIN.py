# -*- coding: utf-8 -*-
"""ALGORITMO V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dPwXCeUgEawgHrc4XK1fqU_7Wk_btcHb
"""

from google.colab import files
from google.colab import drive
drive.mount('/content/gdrive')

from time import time
start_nb = time()
start = time()

import pandas as pd
import re
from nltk import word_tokenize
import string
import nltk
nltk.download('punkt')

dataset = pd.read_csv('//content/gdrive/My Drive/TESIS/dataset.csv',
                      sep=';', encoding='utf-8-sig')
dataset_size = len(dataset)

dataset.head()

def preprocess(doc):
  
    strong = re.compile('<strong>.*?</strong>')
    doc = re.sub(strong, '', doc)

    etp = re.compile('<p>.*?.</p>') # Eliminaci贸n de contenido de la etiqueta <p>.
    doc = re.sub(etp, '', doc)

    pre = re.compile('<pre>|</pre>') # Eliminaci贸n de tag <pre>.
    doc = re.sub(pre, '', doc)

    tag = re.compile('[\s]*<code>|</code>[\s]*') # Eliminaci贸n de tag <code> y espacios.
    doc = re.sub(tag, '', doc)

    etp1 = re.compile('[\S]*<code>|</code>[\S]*') # Eliminaci贸n de contenido de la etiqueta <p>.
    doc = re.sub(etp1, '', doc)

    sigM = re.compile('&lt;') # Reemplazar la entidad &lt; por caracter <.
    doc = re.sub(sigM, '<', doc)

    sigm = re.compile('&gt;') # Reemplazar la entidad &gt; por caracter >.
    doc = re.sub(sigm, '>', doc)

    com = re.compile('&quot;') # Reemplazar la entidad &gt; por caracter >.
    doc = re.sub(com, '"', doc)

    amp = re.compile('&amp;') # Reemplazar la entidad &gt; por caracteror >.
    doc = re.sub(amp, '&', doc)

    p = re.compile('<.*?>') # Reemplazar la entidad &gt; por caracteror >.
    doc = re.sub(p, '', doc)

    doc = doc.lower()  # Minuscula todo el texto.
    #doc = word_tokenize(doc)  # Dividir en palabras.
    # doc = [w for w in doc if w.isalpha()]  # Eliminar numbers and punctuation.
    return doc

dataset['p_body'] = dataset.apply(
        lambda dataset: preprocess(dataset['Attribute:Body']), axis=1)

dataset.iloc[:,[2,5]]

print(dataset['p_body'][0])

df = dataset
df

import numpy as np

def levenshtein_ratio_and_distance(s, t, ratio_calc = False):
    """ levenshtein_ratio_and_distance:
        Calculates levenshtein distance between two strings.
        If ratio_calc = True, the function computes the
        levenshtein distance ratio of similarity between two strings
        For all i and j, distance[i,j] will contain the Levenshtein
        distance between the first i characters of s and the
        first j characters of t
    """
    # Initialize matrix of zeros
    rows = len(s)+1
    cols = len(t)+1
    distance = np.zeros((rows,cols),dtype = int)


    for i in range(1, rows):
        for k in range(1,cols):
            distance[i][0] = i
            distance[0][k] = k   
    for col in range(1, cols):
        for row in range(1, rows):
            if s[row-1] == t[col-1]:
                cost = 0 
            else:
               
                if ratio_calc == True:
                    cost = 2
                else:
                    cost = 1
            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions
                                 distance[row][col-1] + 1,          # Cost of insertions
                                 distance[row-1][col-1] + cost)     # Cost of substitutions
    if ratio_calc == True:
       
        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))
        return Ratio
    else:
        return "The strings are {} edits away".format(distance[row][col])

queryLev = dataset['p_body'][1]
queryLev

queryLev2 = dataset['p_body'][887]
queryLev2

Str1 = dataset['p_body'][386]
Str2 = dataset['p_body'][532]
Distance = levenshtein_ratio_and_distance(Str1.lower(),Str2.lower())
print(Distance)
Ratio = levenshtein_ratio_and_distance(Str1.lower(),Str2.lower(),ratio_calc = True)
print(Ratio)

"""**FUZZY** **WUZZY**"""

!pip install fuzz
!pip install fuzzywuzzy

from fuzzywuzzy import process
st = "apple inc"
strOptions = ["Apple Inc.","apple park","apple incorporated","iphone","apple inc"]
Ratios = process.extract(st,strOptions)
print(Ratios)
# You can also select the string with the highest matching percentage
highest = process.extractOne(st,strOptions)
print(highest)

from fuzzywuzzy import fuzz
Str1 =  dataset['p_body'][37]
Str2 = dataset['p_body'][38]
Ratio = fuzz.ratio(Str1.lower(),Str2.lower())
Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())
Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)
Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)
print(Ratio)
print(Partial_Ratio)
print(Token_Sort_Ratio)
print(Token_Set_Ratio)

Str1= dataset['p_body'][37]
Str2 = dataset['p_body'][38]
Ratios = process.extract(Str1,Str2)
print(Ratios)
# You can also select the string with the highest matching percentage
highest = process.extractOne(Str1,Str2)
print(highest)



#headers = ['Attribute:Id', 'ID', 'is_similar', 'alzahrani']
#dtypes = {'Attribute:Id': 'str', 'ID': 'str', 'is_similar': 'str', 'alzahrani': 'str'}
#fz = pd.read_csv('//content/gdrive/My Drive/Leveinshtein.csv', sep=';', dtype=dtypes)


fz = pd.read_csv('//content/gdrive/My Drive/Leveinshtein.csv',
                   sep=';', encoding='utf-8-sig')

fz.head()

fz = fz.fillna(0)

fz[0:13]

fz.dtypes

# Save Model Using Pickle
import pandas
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
import pickle

array = fz.values
X = array[:,0:4]
Y = array[:,2]

X

Y

seed = 7
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.20, random_state=seed)
# Fit the model on training set
model = LogisticRegression()
model.fit(X_train, Y_train)
# save the model to disk
filename = 'pickle.sav'
pickle.dump(model, open(filename, 'wb'))

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, Y_test)
print(result)

fz['wolf']

# data = run_calculations(text_to_analyze)
# df = pd.DataFrame(data, columns=['soes_id', 'soen_id', 'is_similar', 'wolf'])
fz

import numpy as np
from sklearn.metrics import roc_curve, auc
# calculate roc curves
fpr, tpr, thresholds = roc_curve(fz['is_similar'], fz['wolf'])
roc_auc = auc(fpr, tpr)

print("Area under the ROC curve : %f" % roc_auc)
from matplotlib import pyplot
# plot the roc curve for the model
pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.', label='Logistic')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

gmeans = np.sqrt(tpr * (1 - fpr))
# locate the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold = %f, G-Mean = %.3f' % (thresholds[ix], gmeans[ix]))
best_thresh = thresholds[ix]
fz['pred'] = np.where(fz["wolf"] > thresholds[ix], 1, 0)
#
# Python script for confusion matrix creation.
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

actual = fz['is_similar']
predicted = fz['pred']
results = confusion_matrix(actual, predicted)
print('Confusion Matrix :')
print(results)
print('Accuracy Score :', accuracy_score(actual, predicted))
print('Report : ')
print(classification_report(actual, predicted))
print('Best Threshold = %f' % (best_thresh))

# Save Model Using joblib
import pandas
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
import joblib

test_size = 0.33
seed = 7
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)
# Fit the model on training set
model = LogisticRegression()
model.fit(X_train, Y_train)
# save the model to disk
filename = 'joblib.sav'
joblib.dump(model, filename)
 
# some time later...
 
# load the model from disk
loaded_model = joblib.load(filename)
result = loaded_model.score(X_test, Y_test)
print(result)

import ast

dataset = pd.read_csv('//content/gdrive/My Drive/TESIS/dataset.csv',
                      sep=';', encoding='utf-8-sig')
dataset_size = len(dataset)

dataset.head()

dataset['p_body'] = dataset.apply(
        lambda dataset: preprocess(dataset['Attribute:Body']), axis=1)

dataset.iloc[:,[2,5]]

headers = ['Attribute:Id', 'Attribute:PostTypeId', 'Attribute:Body', 'Attribute:Title','Attribute:Tags','p_body']
dtypes = {'Attribute:Id': 'str','Attribute:PostTypeId': 'int', 'Attribute:Body': 'str', 'Attribute:Title': 'str', 'Attribute:Tags': 'str', 'p_body': 'str' }
#fz = pd.read_csv('//content/gdrive/My Drive/Leveinshtein.csv', sep=';', dtype=dtypes)

parsed = ast.parse(dataset.read['p_body'][2])

ast.dump(parsed)

class Visitor(ast.NodeVisitor):
  def visit(self, node):
print (node)
    super (Visitor, self).visit(node)



v = Visitor()
v.visit(parsed)

parsed

import astor

astor.dump_tree(parsed.body[0])

!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive

!pip install matplotlib-venn

pip install astpath

import ast

code = " def add(x, y):"

module = ast.parse(code)
print module.body[0]

from pprint import pprint

def main():
with open("df", "r") as source:
tree = ast.parse(source.read())

!pip install astpath

from os import path, walk

import pickle

import ast
from .astpath import astpath
from .suffix_tree import SuffixTree

class CodePathsStore:

  def __init__(self, codebase_path):
    self.codebase_path = codebase_path

    self.code_paths_filepath = path.join(codebase_path, ".codeDetectPaths")
    self.code_paths_filepath = path.join(codebase_path, ".codeDetectPaths.pkl")

    if not path.isfile(self.code_paths_filepath):
      self.make_code_paths_file()

    self.paths = self.get_code_paths_from_file()

def make_code_paths_file(self):
    paths = []
    paths = {}
    for root, dirs, files in walk(self.codebase_path):
      for filename in files:
        if filename.endswith(".py"):
          filename_path = path.join(root, filename)
          filepaths = ASTPath(filename_path).paths
          string_paths = ""
          for p in filepaths:
            ind = self.find_index_of_path(p, paths)
            if ind > -1:
              if filename_path not in paths[ind]["files"]:
                paths[ind]["files"].append(filename_path)
            else:
              paths.append({"path": p, "files": [filename_path]})
            string_paths += p

          paths[filename_path] = SuffixTree(string_paths)


    with open(self.code_paths_filepath, "w") as code_paths_file:
      code_paths_file.writelines([treepath["path"] + " " + ",".join(treepath["files"]) + "\n" for treepath in paths])
    with open(self.code_paths_filepath, "wb") as code_paths_file:
      pickle.dump(paths, code_paths_file)

def get_code_paths_from_file(self):
    paths = []
    paths = {}

    with open(self.code_paths_filepath) as code_paths_file:
      for line in code_paths_file:
        path_string, files_string = line.split()
        files = files_string.split(',')
        paths.append({"path": path_string, "files": files})
    file2 = open(self.code_paths_filepath, 'rb')
    paths = pickle.load(file2)
    file2.close()

    return paths

def find_index_of_path(self, treepath, paths=None):
    if paths is None:
      paths = self.paths
    return next((ind for (ind, dic) in enumerate(paths) if dic["path"] == treepath), -1)

paths = ASTPath.get_paths(code)

    #matches = {}

    for treepath in paths:

      match_files = None
      for path_dic in self.code_paths_store.paths:
        if path_dic["path"].endswith(treepath):
          match_files = path_dic["files"]
          break
      match_files = []
      for _filename, _suffixtree in self.code_paths_store.paths.items():
        if _suffixtree.find_substring(treepath) > -1:
          match_files.append(_filename) 


      if match_files is not None:
        for filename in match_files:
          if filename not in matches:
            matches[filename] = 1
          else:
            matches[filename] += 1
    number_of_paths = len(paths)
    match_percentages = {}
    for filename, match_count in matches.items():
      match_percentage = (float(match_count) * 100) / number_of_paths
      match_percentages[filename] = match_percentage
    return match_percentages
 37